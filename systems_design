Numerous researchers have developed their own dataset and prompt permutations to showcase the ease with which direct prompt injections are able to successfully break the LLMs they are working with, with the intention of exfiltrating data and leaking sensitive information about the system’s design.  The term “jailbreaking” is often used to describe this game of cat-and-mouse between the adversary and the systems designers of these LLMs.
